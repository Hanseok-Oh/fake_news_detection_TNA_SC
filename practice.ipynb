{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Trump administration will delay tariffs on cars and car part imports for up to six months as it negotiates trade deals with the European Union and Japan.', 'In a proclamation Friday, Trump said he directed U.S.Trade Representative Robert Lighthizer to seek agreements to “address the threatened impairment” of national security from car imports.', 'Trump could choose to move forward with tariffs during the talks.', '“United States defense and military superiority depend on the competitiveness of our automobile industry and the research and development that industry generates,” White House press secretary Sarah Huckabee Sanders said in a statement.', '“The negotiation process will be led by United States Trade Representative Robert Lighthizer and, if agreements are not reached within 180 days, the President will determine whether and what further action needs to be taken.']\n"
     ]
    }
   ],
   "source": [
    "text = \"The Trump administration will delay tariffs on cars and car part imports for up to six months as it negotiates trade deals with the European Union and Japan. In a proclamation Friday, Trump said he directed U.S.Trade Representative Robert Lighthizer to seek agreements to “address the threatened impairment” of national security from car imports. Trump could choose to move forward with tariffs during the talks. “United States defense and military superiority depend on the competitiveness of our automobile industry and the research and development that industry generates,” White House press secretary Sarah Huckabee Sanders said in a statement. “The negotiation process will be led by United States Trade Representative Robert Lighthizer and, if agreements are not reached within 180 days, the President will determine whether and what further action needs to be taken.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x000002275221EBF8>, {})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guru99 => guru99\nis => is\na => a\nkind => kind\nof => of\nlearning => learning\nexperiences => experience\n. => .\n['guru99', 'is', 'a', 'kind', 'of', 'learning', 'experience', '.']\nguru99 => guru99\nis => be\na => a\nkind => kind\nof => of\nlearning => learn\nexperiences => experience\n. => .\n['guru99', 'be', 'a', 'kind', 'of', 'learn', 'experience', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from collections import defaultdict\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "print(tag_map)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "\n",
    "text1 = \"guru99 is a kind of learning experiences.\"\n",
    "\n",
    "def token_lemma1(text): # token에 is, 같은 애들을 be 로 변환 시키지 않음\n",
    "    results = []\n",
    "    tokens = word_tokenize(text)\n",
    "    lemma_function = WordNetLemmatizer()\n",
    "    for token in tokens:\n",
    "        lemma = lemma_function.lemmatize(token)\n",
    "        print(token, \"=>\", lemma)\n",
    "        results.append(lemma)\n",
    "    return results\n",
    "\n",
    "def token_lemma2(text): # token에 is, 같은 애들을 be 로 변환시킴. 현재는 이걸로 쓸거임\n",
    "    results = []\n",
    "    tokens = word_tokenize(text)\n",
    "    lemma_function = WordNetLemmatizer()\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        lemma = lemma_function.lemmatize(token, tag_map[tag[0]])\n",
    "        print(token, \"=>\", lemma)\n",
    "        results.append(lemma)\n",
    "    return results\n",
    "\n",
    "print(token_lemma1(text1))\n",
    "print(token_lemma2(text1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The => The\nTrump => Trump\nadministration => administration\nwill => will\ndelay => delay\ntariffs => tariff\non => on\ncars => car\nand => and\ncar => car\npart => part\nimports => import\nfor => for\nup => up\nto => to\nsix => six\nmonths => month\nas => a\nit => it\nnegotiates => negotiates\ntrade => trade\ndeals => deal\nwith => with\nthe => the\nEuropean => European\nUnion => Union\nand => and\nJapan => Japan\n. => .\nIn => In\na => a\nproclamation => proclamation\nFriday => Friday\n, => ,\nTrump => Trump\nsaid => said\nhe => he\ndirected => directed\nU.S.Trade => U.S.Trade\nRepresentative => Representative\nRobert => Robert\nLighthizer => Lighthizer\nto => to\nseek => seek\nagreements => agreement\nto => to\n“ => “\naddress => address\nthe => the\nthreatened => threatened\nimpairment => impairment\n” => ”\nof => of\nnational => national\nsecurity => security\nfrom => from\ncar => car\nimports => import\n. => .\nTrump => Trump\ncould => could\nchoose => choose\nto => to\nmove => move\nforward => forward\nwith => with\ntariffs => tariff\nduring => during\nthe => the\ntalks => talk\n. => .\n“ => “\nUnited => United\nStates => States\ndefense => defense\nand => and\nmilitary => military\nsuperiority => superiority\ndepend => depend\non => on\nthe => the\ncompetitiveness => competitiveness\nof => of\nour => our\nautomobile => automobile\nindustry => industry\nand => and\nthe => the\nresearch => research\nand => and\ndevelopment => development\nthat => that\nindustry => industry\ngenerates => generates\n, => ,\n” => ”\nWhite => White\nHouse => House\npress => press\nsecretary => secretary\nSarah => Sarah\nHuckabee => Huckabee\nSanders => Sanders\nsaid => said\nin => in\na => a\nstatement => statement\n. => .\n“ => “\nThe => The\nnegotiation => negotiation\nprocess => process\nwill => will\nbe => be\nled => led\nby => by\nUnited => United\nStates => States\nTrade => Trade\nRepresentative => Representative\nRobert => Robert\nLighthizer => Lighthizer\nand => and\n, => ,\nif => if\nagreements => agreement\nare => are\nnot => not\nreached => reached\nwithin => within\n180 => 180\ndays => day\n, => ,\nthe => the\nPresident => President\nwill => will\ndetermine => determine\nwhether => whether\nand => and\nwhat => what\nfurther => further\naction => action\nneeds => need\nto => to\nbe => be\ntaken => taken\n. => .\n[['The', 'Trump', 'administration', 'will', 'delay', 'tariff', 'on', 'car', 'and', 'car', 'part', 'import', 'for', 'up', 'to', 'six', 'month', 'a', 'it', 'negotiates', 'trade', 'deal', 'with', 'the', 'European', 'Union', 'and', 'Japan', '.'], ['In', 'a', 'proclamation', 'Friday', ',', 'Trump', 'said', 'he', 'directed', 'U.S.Trade', 'Representative', 'Robert', 'Lighthizer', 'to', 'seek', 'agreement', 'to', '“', 'address', 'the', 'threatened', 'impairment', '”', 'of', 'national', 'security', 'from', 'car', 'import', '.'], ['Trump', 'could', 'choose', 'to', 'move', 'forward', 'with', 'tariff', 'during', 'the', 'talk', '.'], ['“', 'United', 'States', 'defense', 'and', 'military', 'superiority', 'depend', 'on', 'the', 'competitiveness', 'of', 'our', 'automobile', 'industry', 'and', 'the', 'research', 'and', 'development', 'that', 'industry', 'generates', ',', '”', 'White', 'House', 'press', 'secretary', 'Sarah', 'Huckabee', 'Sanders', 'said', 'in', 'a', 'statement', '.'], ['“', 'The', 'negotiation', 'process', 'will', 'be', 'led', 'by', 'United', 'States', 'Trade', 'Representative', 'Robert', 'Lighthizer', 'and', ',', 'if', 'agreement', 'are', 'not', 'reached', 'within', '180', 'day', ',', 'the', 'President', 'will', 'determine', 'whether', 'and', 'what', 'further', 'action', 'need', 'to', 'be', 'taken', '.']]\n"
     ]
    }
   ],
   "source": [
    "def lemma_whole(sentences):\n",
    "    lemma_data = []\n",
    "    for sent in sentences:\n",
    "        lemma_sent = token_lemma1(sent)\n",
    "        lemma_data.append(lemma_sent)\n",
    "    return lemma_data\n",
    "\n",
    "lemma_content = lemma_whole(sentences)\n",
    "print(lemma_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Trump', 'administration', 'delay', 'tariff', 'car', 'car', 'part', 'import', 'six', 'month', 'negotiates', 'trade', 'deal', 'European', 'Union', 'Japan'], ['In', 'proclamation', 'Friday', 'Trump', 'said', 'directed', 'U.S.Trade', 'Representative', 'Robert', 'Lighthizer', 'seek', 'agreement', 'address', 'threatened', 'impairment', 'national', 'security', 'car', 'import'], ['Trump', 'could', 'choose', 'move', 'forward', 'tariff', 'talk'], ['United', 'States', 'defense', 'military', 'superiority', 'depend', 'competitiveness', 'automobile', 'industry', 'research', 'development', 'industry', 'generates', 'White', 'House', 'press', 'secretary', 'Sarah', 'Huckabee', 'Sanders', 'said', 'statement'], ['The', 'negotiation', 'process', 'led', 'United', 'States', 'Trade', 'Representative', 'Robert', 'Lighthizer', 'agreement', 'reached', 'within', 'day', 'President', 'determine', 'whether', 'action', 'need', 'taken']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def stopword(sentences):\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    added_stopword = ['“', '”', '.', ',']\n",
    "    results = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        wordsFiltered = []\n",
    "        for w in sent:\n",
    "            if w not in stopWords and w not in added_stopword and not w.isdigit():\n",
    "                wordsFiltered.append(w)\n",
    "        results.append(wordsFiltered)\n",
    "    \n",
    "    print(results)\n",
    "    return results\n",
    "\n",
    "stopped_content = stopword(lemma_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n{'my', 'into', 'again', 'won', 'himself', 'other', \"mustn't\", 'just', 'she', \"it's\", \"don't\", 'herself', 'doing', 'down', \"she's\", 'until', 'needn', 'are', 'me', 'yourselves', 'with', 'those', 'when', \"you're\", 'myself', 'there', 'nor', 'own', 'but', \"wasn't\", 'because', 'm', \"mightn't\", 't', 'then', 'during', 'under', 'd', 'where', 'your', 'or', 'over', 'how', 'it', 'aren', \"that'll\", 'we', 'hers', 'be', 'its', 'being', \"you've\", 'was', 'do', 'as', 'can', 'whom', 'her', 'theirs', 'from', 'of', 'they', 'themselves', 'if', 'mustn', 'weren', \"wouldn't\", 'their', 'that', 'does', 'no', 'between', 'only', 'too', 'to', 'each', 'you', 'these', 'which', 'a', \"won't\", 'them', 'the', 'o', 'is', 've', 'while', 'our', 'couldn', \"shan't\", 'up', 'further', 'not', 'who', 'wasn', 'been', 'same', 'by', \"shouldn't\", 'shan', \"didn't\", 'once', 'have', 'on', 'above', 'didn', 'wouldn', 'will', 'through', \"hasn't\", 'and', 'were', \"aren't\", 'why', \"doesn't\", \"needn't\", 're', 'after', 'for', \"isn't\", 'this', 'hadn', \"should've\", 'don', 'having', 'hasn', 'am', 'what', 'doesn', 'so', 'had', 'y', 'than', 'out', 'most', 'now', \"hadn't\", 'yourself', 'has', 'isn', 'such', 's', \"weren't\", 'his', 'very', \"you'd\", \"haven't\", 'ain', 'both', 'below', 'ma', 'mightn', 'haven', \"couldn't\", 'yours', 'in', 'should', 'ours', 'an', 'few', 'i', 'ourselves', 'before', 'any', \"you'll\", 'more', 'here', 'itself', 'about', 'off', 'against', 'did', 'at', 'him', 'all', 'll', 'some', 'shouldn', 'he'}\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "print(len(stopWords))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('The', 'DT'), ('Trump', 'NNP'), ('administration', 'NN'), ('delay', 'NN'), ('tariff', 'NN'), ('car', 'NN'), ('car', 'NN'), ('part', 'NN'), ('import', 'NN'), ('six', 'CD'), ('month', 'NN'), ('negotiates', 'NNS'), ('trade', 'VBP'), ('deal', 'NN'), ('European', 'NNP'), ('Union', 'NNP'), ('Japan', 'NNP')], [('In', 'IN'), ('proclamation', 'NN'), ('Friday', 'NNP'), ('Trump', 'NNP'), ('said', 'VBD'), ('directed', 'VBN'), ('U.S.Trade', 'NNP'), ('Representative', 'NNP'), ('Robert', 'NNP'), ('Lighthizer', 'NNP'), ('seek', 'JJ'), ('agreement', 'NN'), ('address', 'NN'), ('threatened', 'VBD'), ('impairment', 'JJ'), ('national', 'JJ'), ('security', 'NN'), ('car', 'NN'), ('import', 'NN')], [('Trump', 'NN'), ('could', 'MD'), ('choose', 'VB'), ('move', 'VB'), ('forward', 'RB'), ('tariff', 'NN'), ('talk', 'NN')], [('United', 'NNP'), ('States', 'NNPS'), ('defense', 'NN'), ('military', 'JJ'), ('superiority', 'NN'), ('depend', 'VBP'), ('competitiveness', 'NN'), ('automobile', 'NN'), ('industry', 'NN'), ('research', 'NN'), ('development', 'NN'), ('industry', 'NN'), ('generates', 'VBZ'), ('White', 'NNP'), ('House', 'NNP'), ('press', 'NN'), ('secretary', 'NN'), ('Sarah', 'NNP'), ('Huckabee', 'NNP'), ('Sanders', 'NNP'), ('said', 'VBD'), ('statement', 'NN')], [('The', 'DT'), ('negotiation', 'NN'), ('process', 'NN'), ('led', 'VBD'), ('United', 'NNP'), ('States', 'NNPS'), ('Trade', 'NNP'), ('Representative', 'NNP'), ('Robert', 'NNP'), ('Lighthizer', 'NNP'), ('agreement', 'NN'), ('reached', 'VBD'), ('within', 'IN'), ('day', 'NN'), ('President', 'NNP'), ('determine', 'NN'), ('whether', 'IN'), ('action', 'NN'), ('need', 'VBP'), ('taken', 'VBN')]]\n"
     ]
    }
   ],
   "source": [
    "def tag_content(contents):\n",
    "    \"\"\"\n",
    "    Tag all words in content\n",
    "    :param contents:(list) processed data\n",
    "    :return: (list) tagged words divided by each sentence\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for content in contents:\n",
    "        tagged_content = pos_tag(content)\n",
    "        results.append(tagged_content)\n",
    "    return results\n",
    "\n",
    "tagged_results = tag_content(stopped_content)\n",
    "print(tagged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'isdigit'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-5e54382d6101>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtagged_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopped_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_final\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m    161\u001b[0m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    117\u001b[0m         )\n\u001b[0;32m    118\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mtagged_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Maps to the specified tagset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'eng'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSTART\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'!HYPHEN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m'!YEAR'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'isdigit'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'igraph'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-188bcde5c48a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0migraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m edges = [['a', 'b'],\n\u001b[0;32m      5\u001b[0m          \u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'igraph'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import igraph\n",
    "\n",
    "\n",
    "edges = [['a', 'b'],\n",
    "         ['a', 'b'],\n",
    "         ['a', 'b'],\n",
    "         ['b', 'a'],\n",
    "         ['a', 'c'],\n",
    "         ['c', 'a'],\n",
    "         ['c', 'd'],\n",
    "         ['c', 'd'],\n",
    "         ['d', 'c'],\n",
    "         ['d', 'c']]\n",
    "\n",
    "# collect the set of vertex names and then sort them into a list\n",
    "vertices = set()\n",
    "for line in edges:\n",
    "    vertices.update(line)\n",
    "vertices = sorted(vertices)\n",
    "\n",
    "# create an empty graph\n",
    "g = igraph.Graph()\n",
    "\n",
    "# add vertices to the graph\n",
    "g.add_vertices(vertices)\n",
    "\n",
    "# add edges to the graph\n",
    "g.add_edges(edges)\n",
    "\n",
    "# set the weight of every edge to 1\n",
    "g.es[\"weight\"] = 1\n",
    "\n",
    "# collapse multiple edges and sum their weights\n",
    "g.simplify(combine_edges={\"weight\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'network'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8b5382dc7b2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"The Trump administration will delay tariffs on cars and car part imports for up to six months as it negotiates trade deals with the European Union and Japan. In a proclamation Friday, Trump said he directed U.S.Trade Representative Robert Lighthizer to seek agreements to “address the threatened impairment” of national security from car imports. Trump could choose to move forward with tariffs during the talks. “United States defense and military superiority depend on the competitiveness of our automobile industry and the research and development that industry generates,” White House press secretary Sarah Huckabee Sanders said in a statement. “The negotiation process will be led by United States Trade Representative Robert Lighthizer and, if agreements are not reached within 180 days, the President will determine whether and what further action needs to be taken.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'network'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import network\n",
    "\n",
    "\n",
    "text = \"The Trump administration will delay tariffs on cars and car part imports for up to six months as it negotiates trade deals with the European Union and Japan. In a proclamation Friday, Trump said he directed U.S.Trade Representative Robert Lighthizer to seek agreements to “address the threatened impairment” of national security from car imports. Trump could choose to move forward with tariffs during the talks. “United States defense and military superiority depend on the competitiveness of our automobile industry and the research and development that industry generates,” White House press secretary Sarah Huckabee Sanders said in a statement. “The negotiation process will be led by United States Trade Representative Robert Lighthizer and, if agreements are not reached within 180 days, the President will determine whether and what further action needs to be taken.\"\n",
    "\n",
    "\n",
    "N = network.Processing()\n",
    "lemed_content = N.lemma_whole(text)\n",
    "print(lemed_content)\n",
    "\n",
    "stopped_content = N.stopword(lemed_content)\n",
    "\n",
    "tagged_results = N.tag_content(stopped_content)\n",
    "print(tagged_results)\n",
    "print('***************************************')\n",
    "\n",
    "# 어떤 태그들만 남길지\n",
    "tag_filter = ['NNP', 'NN', 'NNS', 'VBP', 'VBD', 'VBN', 'JJ', 'RB', 'VB']\n",
    "\n",
    "selected_results = N.select_results(tagged_results, tag_filter)\n",
    "print(selected_results)\n",
    "\n",
    "final_result = N.create_cooc_mat(selected_results)\n",
    "\n",
    "final_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,3\n[('Trump', 'administration'), ('Trump', 'delay'), ('Trump', 'tariff'), ('Trump', 'car')]\n[]\n[['Trump', 'administration'], ['Trump', 'delay'], ['Trump', 'tariff'], ['Trump', 'car']]\n"
     ]
    }
   ],
   "source": [
    "X = ','.join(str(a) for a in [1,2,3] )\n",
    "print(X)\n",
    "\n",
    "list_key = [('Trump', 'administration'), ('Trump', 'delay'), ('Trump', 'tariff'), ('Trump', 'car')]\n",
    "empty_list = []\n",
    "# for w1, w2 in list_key:\n",
    "#     # w = ','.join(s for s in k)\n",
    "#     w = [w1, w2]\n",
    "#     empty_list.append(w)\n",
    "\n",
    "w_oneline = [[w1, w2] for w1, w2 in list_key]\n",
    "\n",
    "print(list_key)\n",
    "print(empty_list)\n",
    "print(w_oneline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
